<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Learning and Tuning ~ LLMs at Work</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha512-NhSC1YmyruXifcj/KFRWoC561YpHpc5Jtzgvbuzx5VozKpWvQ+4nXhPdFgmx8xqexRcpAglTj9sIBWINXa8x5w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="./static/style.css" type="text/css">
<link rel="stylesheet" href="./static/pygments.css" type="text/css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <link rel="shortcut icon" href="./static/icon.ico" /> -->
</head>
<body>
<header>
  <div class="title"><span>Chapter 4: Learning and Tuning</span></div>
  <input type="checkbox" id="toggle">
  <label class="hamburger" for="toggle"><span></span><span></span><span></span></label>
  <ul class="menu">
    <li><a href="./index.html">Front Cover</a></li>
    <li><a href="./table-of-contents.html">Table of Contents</a></li>
    <li><a href="./roadmap.html">üó∫Ô∏è Roadmap</a></li>
    <li><a href="https://github.com/vladris/llm-book/issues/new">üì£ Feedback</a></li>
    <li><a href="https://tinyletter.com/vladris">‚úâÔ∏è Subscribe for updates</a></li>
  </ul>
</header>
<article>
<h1>Learning and Tuning</h1>

<p><img src="images/04/cover.png" alt="Learning and Tuning"></p>

<p>In this chapter:</p>

<ul>
<li>Rewriting text in different styles.</li>
<li>Running a text adventure game.</li>
<li>Performing sentiment analysis.</li>
<li>Implementing a Q&amp;A over a dataset.</li>
</ul>

<p>As we discussed large language models, we described them as huge neural networks
with billions of parameters, that are very expensive to train. Training such a
model is a big undertaking well outside the scope of this book. We talked a lot
about using these model as they are provided by OpenAI, meaning already trained.
As a reminder, GPT stands for <em>Generative Pre-trained Transformer</em>. Note the
‚Äúpre-trained‚Äù.</p>

<p>That said, we <em>can</em> teach an old model new tricks. This chapter is all about how
a large model can learn. The learning we‚Äôll cover is different than training as
it is used in the context of neural networks. We won‚Äôt be starting from scratch
and attempting to train the billions of parameters in a model. Instead, we will
start with an already trained model and see what we can do from there.</p>

<p>First, there is <em>zero-shot learning</em>. This means the model might be able to
perform a task it was not specifically trained for. We prompt it to do
something, and it does this by nature of its knowledge of language and
relationship between words. We‚Äôll see a couple of examples of this and what it
means.</p>

<p>Next, we have <em>one-shot learning</em>. With one-shot learning, we provide a single
example. Once given this example, the model can then accomplish the task.</p>

<p>Sometimes the scenario is more complex, and one-shot learning is not enough. In
this case, we have <em>few-shot learning</em>: this means providing several examples
from which the model can derive (learn) what it is supposed to reply.</p>

<p>Zero-, one-, and few-shot learning are all similar. They‚Äôre common techniques to
get more out a pre-trained model, simply by prompting. In some situations, this
is not enough. In the second part of this chapter, we‚Äôll introduce
<em>fine-tuning</em>. This is a more involved process where we can provide any number
of questions and answers to a model which will help tune its responses.</p>

<p>OpenAI used to only support fine-tuning for their <code>ada</code>, <code>babbage</code>, <code>curie</code>, and
<code>davinci</code> models, with <code>davinci</code> being the most powerful of the bunch. More
recently, OpenAI started providing a fine-tunable version of GPT-3.5 Turbo ‚Äì
<code>gpt-3.5-turbo-instruct</code>. We will implement a Q&amp;A fine-tuning a model, then
talk about some of the tradeoffs when using fine-tuning.</p>

<p>Let‚Äôs see what zero-shot learning is about.</p>

<h2 id="zero-shot-learning">Zero-shot learning</h2>

<p>Let‚Äôs start with an example: a large language model trained on a wide range of
texts can perform a translation task from one language to another without ever
having been explicitly trained on that particular translation pair. The model
can be given a prompt in the source language, along with a description of the
target language, and it will generate a translation based on its understanding
of language.</p>

<p>Listing 4.1 shows an example of this (we can save this as <code>translate.json</code>).</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You are an English to French translator.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Translate this to French: {{text}}&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 4.1: English-to-French prompt template.</em></p>

<p>Note we are not modifying any parameter in listing 4.1, just providing the
<code>messages</code>. For this scenario we don‚Äôt have to adjust <code>temperature</code>,
<code>max_tokens</code> etc. If we don‚Äôt mention them, the default values will be used.</p>

<p>We‚Äôre relying on the prompt templating we set up in Chapter 3. We‚Äôll be using
<code>gpt-3.5-turbo</code>, the chat completion model. This template contains a
<code>system</code> message, telling the model to act as a translator, and a prompt to
translate text to French. The actual text is replaced at runtime.</p>

<p>Listing 4.2 shows how we can use this to make an OpenAI API call.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span>
    <span class="s1">&#39;translate.json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">completion</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s2">&quot;Aren&#39;t large language models amazing?&quot;</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.2: English-to-French translation using chat completion.</em></p>

<p>We instantiate a <code>ChatTemplate</code> from the template file we defined in listing 4.1
and call the <code>completion()</code> function, setting the text to <em>‚ÄúAren‚Äôt large
language models amazing?‚Äù</em>. Running this code should print something like <em>‚ÄúLes
grands mod√®les de langage ne sont-ils pas incroyables?‚Äù</em></p>

<p>So how is this zero-shot learning? Well, <code>gpt-3.5-turbo</code> was not specifically
trained for English-to-French translation. Historically, natural language
processing models designed for translation are trained for the task. In our
case, we would train a model on English-to-French translations. This model has
not been trained for this specific task, but it can still perform it very well
when prompted to do so.</p>

<blockquote>
<p><strong>Definition</strong>: <em>zero-shot learning</em> refers to the ability of a language model
to perform a task without any explicit training data or examples for that task.
Instead, the model can use its knowledge of language and the relationships
between words to perform the task based on a textual description or prompt.</p>
</blockquote>

<p>Another example of zero-shot learning is the ability of large language models to
answer questions. These models are not trained specifically for Q&amp;A, rather a
large corpus of data, large enough that it can both understand the semantics of
a question and rely on the data it has been trained on to answer it.</p>

<p>Note since the models are <em>not</em> trained for Q&amp;A specifically, rather just to
predict the most likely next word and the one after that and so on, they are
prone to hallucinating (making things up), especially when they don‚Äôt have an
answer handy.</p>

<p>Zero-shot learning is impressive from the perspective of what models can achieve
‚Äì without us having to do anything, we see large language models exhibit some
powerful capabilities. On the other hand, zero-shot learning means we literally
don‚Äôt have to do anything, just ask. There are some limits to what can be done
with zero-shot learning. At some point, we need to show some examples. Enter
one-shot learning.</p>

<h2 id="one-shot-learning">One-shot learning</h2>

<p>With one-shot learning, we provide the large language model with a single
example, which helps it better understand our prompt.</p>

<blockquote>
<p><strong>Definition</strong>: <em>One-shot learning</em> refers to the ability of a large language
model to learn a new task or solve a new problem from only a single example.</p>
</blockquote>

<p>In other words, instead of requiring a large dataset of labeled examples to
train on, a language model with one-shot learning capabilities can quickly adapt
to a new task or context with minimal training data.</p>

<p>This is done through <em>transfer learning</em>, where the model leverages its prior
knowledge and pre-trained representations to quickly learn new concepts or
patterns. By doing so, the model can generalize its understanding to new
situations, even when faced with limited data.</p>

<p>Of course, there is not a lot we can teach with a single example. On the other
hand, sometimes it is easier to show an example rather than trying to describe
what we want. For example, listing 4.3 shows a prompt for rewriting text in a
different style.</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Here is some sample text: {{sample}} Rewrite the following text in the same style as the sample: {{text}}&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 4.3: Text rewriting template.</em></p>

<p>In this example, we provide some sample text and ask the model to rewrite
another text in the same style as the sample. Let‚Äôs save this as <code>rewrite.json</code>.
We could‚Äôve provide a detailed description of the style instead, but that would
likely require more prompt tuning as we ourselves would have to understand and
explain the subtle elements of what a writing style entails. Rather than doing
that, we simply show the text in the style we want and let the large language
model figure out what it means.</p>

<p>Let‚Äôs put this to work. Listing 4.4 shows how we can use this template for rewriting.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s1">&#39;rewrite.json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">completion</span><span class="p">({</span>
    <span class="s1">&#39;sample&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Deep in the shady sadness of a vale</span>
<span class="s1">Far sunken from the healthy breath of morn,</span>
<span class="s1">Far from the fiery noon, and eve&#39;s one star,</span>
<span class="s1">Sat gray-hair&#39;d Saturn, quiet as a stone,</span>
<span class="s1">Still as the silence round about his lair;</span>
<span class="s1">Forest on forest hung about his head</span>
<span class="s1">Like cloud on cloud. No stir of air was there,</span>
<span class="s1">Not so much life as on a summer&#39;s day</span>
<span class="s1">Robs not one light seed from the feather&#39;d grass,</span>
<span class="s1">But where the dead leaf fell, there did it rest.</span>
<span class="s1">A stream went voiceless by, still deadened more</span>
<span class="s1">By reason of his fallen divinity</span>
<span class="s1">Spreading a shade: the Naiad &#39;mid her reeds</span>
<span class="s1">Press&#39;d her cold finger closer to her lips.&#39;&#39;&#39;</span><span class="p">,</span>
    <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s2">&quot;Let&#39;s start with an example: a large language model trained </span><span class="se">\</span>
<span class="s2">on a wide range of texts can perform a translation task from one language to </span><span class="se">\</span>
<span class="s2">another without ever having been explicitly trained on that particular </span><span class="se">\</span>
<span class="s2">translation pair. The model can be given a prompt in the source language, </span><span class="se">\</span>
<span class="s2">along with a description of the target language, and it will generate a </span><span class="se">\</span>
<span class="s2">translation based on its understanding of language.&quot;</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.4: Rewriting chat completion.</em></p>

<p>We provide as sample a few lines from John Keats‚Äô Hyperion. The text is a
paragraph from earlier in this chapter. Let‚Äôs see how Keats would explain
zero-shot language translation. Listing 4.5 shows a possible reply.</p>
<div class="highlight"><pre><span></span>Deep in the vast expanse of knowledge,
Far removed from the familiar words of men,
Far from the common phrases, and dialects of foreign lands,
Resides the mighty language model, still as a statue,
Silent, amidst the stillness of its lair;
Knowledge upon knowledge stored within its head,
A library of texts that stretch beyond the sky.
No disturbance from the outer world can penetrate,
The silence, unbroken, like a winter&#39;s night,
But when prompted in one tongue, and guided to another,
It will weave language, a master of communication,
Based on its vast understanding of the spoken word.
</pre></div>

<p><em>Listing 4.5: One-shot rewrite by large language model.</em></p>

<p>Admittedly, Keats is a famous poet, so we could‚Äôve asked the model to rewrite in
the style of John Keats, but this example demonstrates how we can sample some
text and have the model adopt the style with a simple example.</p>

<p>Another good use-case for one-shot learning is to format of the output. If we
have some very specific formatting requirements, showing an example rather than
describing the format makes for simpler prompts.</p>

<p>Listing 4.6 (<code>xml.json</code>) shows an example of us wanting an XML reply when
prompting the model for facts about some entity.</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Give me some facts about Saturn&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&lt;FACTS entity=\&quot;Saturn\&quot;&gt;&lt;FACT&gt;Named after the Roman god of agriculture and harvest&lt;/FACT&gt;&lt;FACT&gt;At least 82 known moons&lt;/FACT&gt;&lt;/FACTS&gt;&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Give me some facts about {{entity}}&quot;</span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 4.6: One-shot formatting template.</em></p>

<p>We expect the answer to be contained within the <code>&lt;FACTS&gt;</code> element. This element
contains an <code>entity</code> attribute specifying the target entity. We then want each
fact to show up inside <code>&lt;FACT&gt;</code> elements. In our prompt, we provide an example
of this as part of the chat conversation ‚Äì we show an example question (facts
about Saturn) and the expected response for that question. We then repeat the
question but for a different entity.</p>

<p>Listing 4.7 shows how we can use this to get formatted facts from a large
language model.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span>
    <span class="s1">&#39;xml.json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">completion</span><span class="p">({</span><span class="s1">&#39;entity&#39;</span><span class="p">:</span> <span class="s1">&#39;Elon Musk&#39;</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.7: Formatted output chat completion.</em></p>

<p>In this case we are asking for facts about Elon Musk. Running this code, I got
the reply in listing 4.8 (your milage may vary as models are non-deterministic).</p>
<div class="highlight"><pre><span></span><span class="nt">&lt;FACTS</span><span class="w"> </span><span class="na">entity=</span><span class="s">&quot;Elon Musk&quot;</span><span class="nt">&gt;&lt;FACT&gt;</span>CEO<span class="w"> </span>of<span class="w"> </span>SpaceX,<span class="w"> </span>Tesla,<span class="w"> </span>Neuralink,<span class="w"> </span>and<span class="w"> </span>The<span class="w"> </span>Boring
Company<span class="nt">&lt;/FACT&gt;&lt;FACT&gt;</span>Net<span class="w"> </span>worth<span class="w"> </span>of<span class="w"> </span>over<span class="w"> </span>$200<span class="w"> </span>billion<span class="w"> </span>as<span class="w"> </span>of
2021<span class="nt">&lt;/FACT&gt;&lt;FACT&gt;</span>Founded<span class="w"> </span>PayPal<span class="nt">&lt;/FACT&gt;&lt;FACT&gt;</span>Has<span class="w"> </span>a<span class="w"> </span>vision<span class="w"> </span>to<span class="w"> </span>colonize<span class="w"> </span>Mars<span class="w"> </span>and
make<span class="w"> </span>humanity<span class="w"> </span>a<span class="w"> </span>multi-planetary<span class="w"> </span>species<span class="nt">&lt;/FACT&gt;&lt;FACT&gt;</span>Has<span class="w"> </span>publicly<span class="w"> </span>stated<span class="w"> </span>concerns
about<span class="w"> </span>the<span class="w"> </span>potential<span class="w"> </span>dangers<span class="w"> </span>of<span class="w"> </span>artificial<span class="w"> </span>intelligence<span class="nt">&lt;/FACT&gt;&lt;/FACTS&gt;</span>
</pre></div>

<p><em>Listing 4.8: One-shot formatting by large language model.</em></p>

<p>The model was able to understand the format we wanted, and the response conforms
to the schema.</p>

<p>In general, consider using one-shot learning when it‚Äôs easier to <em>show</em> than
<em>tell</em>. If it takes fewer tokens to give an example or the example is clearer
than a description, the large language model should be able to easily understand
it. We saw it rewrite text in the style of John Keats from a few lines of
poetry, and we saw it output formatted XML from a single example of a similar
question/answer.</p>

<p>Of course, there is only so much we can convey with one example. Sometimes, we
need to provide a set of examples for the model to understand our goals.</p>

<h2 id="few-shot-learning">Few-shot learning</h2>

<p>You can think of few-shot learning as an extended version of one-shot learning ‚Äì
instead of giving a single example to the large language model, we provide
several examples.</p>

<blockquote>
<p><strong>Definition</strong>: <em>Few-shot learning</em> refers to the ability of a large language
model to learn a new task or solve a new problem from a small set of examples.</p>
</blockquote>

<p>In some cases, a single example might not provide enough information to give us
the desired result. We might end up here as we tune our prompt ‚Äì we first
attempt a zero-shot prompt, but we don‚Äôt get back quite what we were expecting.
We then try a one-shot prompt, but even that misses the mark. In that case, we
can try providing multiple examples and hopefully the model can better infer
what we want.</p>

<p>Figure 4.1 illustrates few-shot learning.</p>

<p><img src="images/04/fig1.png" alt="Figure 4.1"></p>

<p><em>Figure 4.1: Few-shot learning.</em></p>

<p>We take the user input and combine it with a set of examples to compose the
prompt we send to the large language model.</p>

<h3>Text adventure</h3>

<p>An example of when we would want to use few-shot learning is when we want to
‚Äúteach‚Äù the model to act as a text adventure game. We‚Äôll show the model a couple
of user actions and responses (e.g. for the user action <code>Look around</code>, the
response is <code>You are in a room with a table and a door</code>). Given a few such
examples, the model should be able to ‚Äúplay‚Äù with us.</p>

<p>In chapter 2 (listing 2.8) we implemented a simple command-line interactive
chat. We can expand that to act as our adventure game by pre-seeding the chat
history with a few-shot learning examples of how we want the model to reply. See
listing 4.9.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;messages&#39;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are a text adventure game.&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Look around&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You are in a room with a table and a door.&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Open door&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;The door is locked.&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Check inventory&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You have a sandwich.&#39;</span><span class="p">}]})</span>

<span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">chat</span><span class="o">.</span><span class="n">template</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;assistant&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;&gt; &#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="o">==</span> <span class="s1">&#39;exit&#39;</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="n">chat</span><span class="o">.</span><span class="n">template</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span>
    <span class="n">message</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">completion</span><span class="p">({})</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">chat</span><span class="o">.</span><span class="n">template</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">role</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">})</span>
</pre></div>

<p><em>Listing 4.9: Large language model-based text adventure.</em></p>

<p>The only thing we added on top of our command line chat from chapter 2 is the
chat history. In listing 2.8, this started out as an empty list. Here, it
contains several examples of user actions and responses. Before we enter the
chat loop, we print the <code>assistant</code> messages to give some context to the user.</p>

<p>If we run this code we can continue ‚Äúplaying‚Äù and getting realistic responses
from the model. Listing 4.10 shows a few actions and replies we could get by
running the code.</p>
<div class="highlight"><pre><span></span>You are in a room with a table and a door.
The door is locked.
You have a sandwich.
&gt; What is on the table?
You see a key lying on the table.
&gt; I use the key to open the door
You use the key to unlock the door and open it. You find yourself in a dark hallway. There are doors on either side of the hallway and a staircase leading up.
&gt; I eat the sandwich
You eat the sandwich and feel slightly less hungry.
</pre></div>

<p><em>Listing 4.10: Example actions and responses.</em></p>

<p>The few-shot in this case are the <code>user</code> and <code>assistant</code> messages we put in
the<code>history</code>. Note we‚Äôre also starting with a <code>system</code> message providing some
context to the large language model. This is pretty impressive! When text
adventures were popular, in the early days of PCs, developers had to hand-craft
each and every interaction. With the help of large language models, we can now
achieve similar results with just a few examples.</p>

<p>This was an example of a generative scenario. We prime the model to generate the
type of responses we would expect from a text adventure game. Let‚Äôs look at
another example in another domain: sentiment analysis.</p>

<h3>Sentiment analysis</h3>

<p>Large language models have extremely powerful language skills, so we can
leverage these for sentiment analysis. Say we want to evaluate the sentiment of
a restaurant review, on a scale from 1 to 5 (and also get some explanation of
why the model assigned the score it did). We can use few-shot learning for this
as shown in listing 4.11 (we can save this as <code>sentiment.json</code>).</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;You are an assistant designed to analyze sentiment of customer reviews. For a given review, you will respond with an assessment of the review. Rate on a scale of 1-5 (5 being highest). Explain why this rating was given&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;We&#39;ve been here several times, and every time it wows us. Everything I&#39;ve ordered I would order again.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5: Short review in which the customer praises the food and mentions being a return customer.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Love the food and have constantly enjoyed their sushi this last year. However, it&#39;s been too often that we&#39;ve waited 45-60 minutes for our food. They take so many to go orders that they neglect their in-house patrons.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2: The customer really enjoys the food but is dissatisfied by the long wait. The fact that the restaurant prioritizes to-go order makes them feel neglected.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;I loooove the sushi here! I&#39;ve ordered from here multiple times and have dined in as well. Both are always great experiences and the sushi is always so yummy.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;5: The customer both dined in and ordered to go multiple times, had a great experience each time, and really enjoys the food.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;{{review}}&quot;</span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 4.11: Few-shot sentiment analysis template.</em></p>

<p>We first use the <code>system</code> message to tell the model how we want it to reply. We
then provide a set of examples, where the <code>user</code> message contains the review
text and the <code>assistant</code> message contains the response, consisting of a score
and an explanation derived from the review text.</p>

<p>We can use this template with our interactive chat as shown in listing 4.12.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s1">&#39;sentiment.json&#39;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;&gt; &#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="o">==</span> <span class="s1">&#39;exit&#39;</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="n">message</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">completion</span><span class="p">({</span><span class="s1">&#39;review&#39;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.12: Sentiment analysis for restaurant reviews.</em></p>

<p>For this particular scenario we don‚Äôt need chat history, as we expect to score
reviews one by one as they are provided. We simply load the template from
<code>sentiment.json</code> and on each call replace the <code>{{review}}</code> parameter with the
actual review. We print the response we get back. Listing 4.13 shows an example
of this interaction.</p>
<div class="highlight"><pre><span></span>&gt; The food was fantastic but the kitchen was slowwwwwwwww and super disorganized. It seemed like they prioritized delivery orders over diners and it was really disappointing. But for the sushi and the prices this place was excellent!!
3: The customer really enjoyed the food and the prices, but is disappointed by the slow and disorganized kitchen. It seems like delivery orders are prioritized over diners, which is frustrating.
</pre></div>

<p><em>Listing 4.13: Example sentiment analysis.</em></p>

<p>We can see the model provides an accurate assessment of the review,
understanding the pros and cons and estimating the sentiment value.</p>

<p>Using few-shot learning, we can enable many scenarios across different domains.
Feeding a small set of examples to the large language model makes it understand
precisely how we want to use it and how it should format its response.</p>

<p>For more complex scenarios, we can combine few-shot learning with prompt
selection. We discussed prompt selection in chapter 3. Depending on the ask, we
can first apply prompt selection to identify which examples we want to inject
into the prompt and pick these from a broader range of possible few-shot
examples. Figure 4.2 shows how this would work.</p>

<p><img src="images/04/fig2.png" alt="Figure 4.2"></p>

<p><em>Figure 4.2: Prompt selection and few-shot learning.</em></p>

<p>This is very similar with the prompt selection flow we looked at in chapter 3:</p>

<ol>
<li><p>We have a selection prompt template and use the large language model to
process the original user ask and determine which template we should use.</p></li>
<li><p>In this case, the templates we are selecting from are few-shot learning
templates containing sets of examples for specific scenarios. We pick the right
template based on the selection prompt, then use that combined with the user ask
to generate the final prompt we send to the model.</p></li>
</ol>

<p>We won‚Äôt go over an implementation as we already have all the building blocks
and putting them together will take too much space. Another powerful combination
is combining the few-shot learning examples with some form of memory ‚Äì more
about this in chapter 5.</p>

<h3>Recap</h3>

<p>We covered zero-shot, one-shot, and few-shot learning. Before moving on, let‚Äôs
quickly recap:</p>

<ul>
<li>Zero-shot learning pretty much means the model can perform a task it was not
specifically trained for. You can start here when engineering your prompt ‚Äì
maybe the pre-trained model has enough knowledge to provide a good answer to
your prompt without additional examples. One example we looked at was
translation between two languages.</li>
<li>One-shot learning means providing one example to the prompt. This comes in
handy when it is easier to show than to tell ‚Äì it can be used to both add
context to the input prompt (our example was giving some Keats verses and
asking the model to rewrite some text in that style) or to describe the output
(we got the model to output in a custom XML format just by showing an
example).</li>
<li>Few-shot learning comes into play when a single example is not sufficient. We
provide the model with a set of examples that help it refine its reply.</li>
</ul>

<p>As we saw before, models have no memory. We need to supply these examples in the
prompt, and we must do it on each subsequent call if we want to get the same
results. There are situations when we‚Äôd rather not do this ‚Äì we would like to
somehow teach the model something and have it stick.</p>

<h2 id="fine-tuning">Fine-tuning</h2>

<p>While training a performant large language model from scratch is prohibitively
expensive, a pre-trained model can be tuned with some domain-specific
information.</p>

<blockquote>
<p><strong>Definition</strong>: <em>Fine-tuning</em> large language models refers to the processing
of adapting a general-purpose model for a specific task or domain. Pre-trained
language models can be trained with additional data to specialize them for
specific application scenarios.</p>
</blockquote>

<p>This process involves an additional training step. After the expensive training
of the model, its (trained) parameters are frozen. During fine-tuning, new
domain-specific parameters are added and trained using a small domain-specific
dataset. The fine-tuning adjusts these parameters, without modifying the
pre-trained parameters at all. This process usually yields better performance on
the new domain-specific tasks.</p>

<p>Fine-tuning is very different than what we covered during the first half of this
chapter, with zero-, one-, few-shot learning. Instead of leveraging a
pre-trained model and including relevant information in the prompt, we are
modifying the model itself based on our data.</p>

<p>We won‚Äôt be using prompts for this, rather a dedicated API. In fact, not all
models can be fine-tuned. We‚Äôve been using <code>gpt-3.5-turbo</code> in a lot of our
examples. <code>gpt-3.5-turbo</code> can‚Äôt be fine-tuned. It used to be that only the older
base models <code>ada</code>, <code>babbage</code>, <code>curie</code>, and <code>davinci</code> supported fine-tuning. More
recently, OpenAI launched <code>gpt-3.5-turbo-instruct</code> with fine-tuning
capabilities.</p>

<p>Pricing for this at the time of writing is shown in table 4.1. As always, you can find the up-to-date pricing at <a href="https://openai.com/pricing">https://openai.com/pricing</a>.</p>

<table>
<thead>
<tr>
<th>MODEL</th>
<th>TRAINING PRICE</th>
<th>INPUT USAGE</th>
<th>OUTPUT USAGE</th>
</tr>
</thead>

<tbody>
<tr>
<td>Babbage</td>
<td>$0.0004 / 1K tokens</td>
<td>$0.0016 / 1K tokens</td>
<td>$0.0016 / 1K tokens</td>
</tr>
<tr>
<td>Davinci</td>
<td>$0.0060 / 1K tokens</td>
<td>$0.0120 / 1K tokens</td>
<td>$0.0120 / 1K tokens</td>
</tr>
<tr>
<td>GPT-3.5 Turbo</td>
<td>$0.0080 / 1K tokens</td>
<td>$0.0120 / 1K tokens</td>
<td>$0.0160 / 1K tokens</td>
</tr>
</tbody>
</table>

<p><em>Table 4.1: Pricing for fine-tuning</em></p>

<p>Two interesting things to note:</p>

<ul>
<li>Pricing is done per 1000 tokens, with different prices for the training
(fine-tuning) part, and subsequent usage of the fine-tuned model.</li>
<li>Pricing increases by an order of magnitude from <code>babbage</code> to <code>davinci</code>. The
best performing model, <code>gpt-3.5-turbo-instruct</code>, is the most expensive.</li>
</ul>

<p>Keep this in mind when using these for a production scenario and find the sweet
spot between cost and model performance. In our example we‚Äôll be using
<code>davinci</code>. In general, the larger the dataset we train with, the better results
we get.</p>

<p>Let‚Äôs walk through the steps of fine-tuning a model to answer questions about a
fictional race (something the model was definitely not trained on). Let‚Äôs say we
have a dataset of the 2023 Pod Racing standings as showing in table 4.2.</p>

<table>
<thead>
<tr>
<th>PILOT</th>
<th>POD</th>
<th>RACE 1</th>
<th>RACE 2</th>
<th>RACE 3</th>
<th>RACE 4</th>
<th>RACE 5</th>
</tr>
</thead>

<tbody>
<tr>
<td>Senn Kava</td>
<td>The Thunderbolt</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>Vix Tor</td>
<td>The Shadow Racer</td>
<td>3</td>
<td>1</td>
<td>2</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>Remy Thal</td>
<td>The Crimson Fang</td>
<td>2</td>
<td>5</td>
<td>3</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>Tira Suro</td>
<td>The Lightning Bolt</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>Kael Voss</td>
<td>The Razor Blade</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>2</td>
<td>3</td>
</tr>
</tbody>
</table>

<p><em>Table 4.2: 2023 Pod Racing standings.</em></p>

<p>5 races happened this season:</p>

<ul>
<li>Tatooine Grand Prix (race 1), won by Senn Kava.</li>
<li>Coruscant Circuit (race 2), won by Vix Tor.</li>
<li>Naboo Invitational (race 3), won by Senn Kava.</li>
<li>Genosis Challenge (race 4), won by Tira Suro.</li>
<li>Bespin Cup (race 5), won by Remy Thal.</li>
</ul>

<p>Let‚Äôs see how we can fine-tune a model to answer questions about this race
season. Note we‚Äôre using a small example to keep things sane ‚Äì we might actually
get away with putting all this information in a prompt and have the model answer
the question just by the additional context. But this is just an example. As the
size of the data scales, fine-tuning becomes more appealing. We also won‚Äôt have
to consume so many tokens with each prompt as we would if we were to pass data
with each prompt as context.</p>

<p>First, we need to prepare the data for training the model.</p>

<h3>Preparing the dataset</h3>

<p>OpenAI expects fine-tuning data to be passed as JSONL. JSONL stands for JSON
Lines, meaning JSON objects separated by newlines. Each JSON object consists of
a <code>prompt</code> and a <code>completion</code>. The <code>prompt</code> in this case is a question, and the
<code>completion</code> is the ideal answer we would like to get from the large language
model.</p>

<p>Listing 4.14 shows an example JSONL format.</p>
<div class="highlight"><pre><span></span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Knock, knock!&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Who&#39;s there?&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="p">{</span><span class="w"> </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Orange&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Orange who?&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="p">{</span><span class="w"> </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Boo&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Boo who?&quot;</span><span class="w"> </span><span class="p">}</span>
</pre></div>

<p><em>Listing 4.14: Sample JSONL.</em></p>

<p>We‚Äôll need to format our race standing data into <code>prompt</code> and <code>completion</code>
pairs. An easy way to do this is to use a large language model for the task.
We‚Äôll fragment our data into smaller chunks (to work around token limits if we
have a very large dataset) and ask <code>gpt-3.5-turbo</code> to come up with some prompts
and answers based on the dataset. We‚Äôll lower the temperature as we want the
model to use the data provided rather than making things up (though this might
make the questions less imaginative). <code>gpt-3.5-turbo</code> will help us fine-tune a
<code>davinci</code> base model!</p>

<p>Listing 4.15 shows the prompt we‚Äôll use to generate these questions and answers.
Let‚Äôs save this a <code>jsonl.json</code>.</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Here is some information about the 2023 Pod Racing season: Race 1 was the Tatooine Grand Prix, won by Senn Kava. Provide 2 prompts and completions based on this data in JSONL format.&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;{\&quot;prompt\&quot;: \&quot;What was the name of the first race in the 2023 Pod Racing season?\&quot;, \&quot;completion\&quot;: \&quot;The Tatooine Grand Prix.\&quot;}\n{\&quot;prompt\&quot;: \&quot;Who won the Tatooine Grand Prix?\&quot;, \&quot;completion\&quot;: \&quot;Senn Kava.\&quot;}&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Here is some information about the 2023 Pod Racing season: {{info}}. Provide {{n}} prompts and completions based on this data in JSONL format.&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>

<p><em>Listing 4.15: Prompt to generate questions and answers.</em></p>

<p>We‚Äôre using one-shot to prime the model to format the response as JSONL. In the
example we are providing a short bit of information and expected JSONL reply for
2 prompts and completions. Next we make the information a parameter (<code>{{info}}</code>)
and the number of prompts and completions we want generated another parameter
(<code>{{n}}</code>).</p>

<p>We can then pass in a chunk of information and a number of prompts and
completions to generate.</p>

<p>Listing 4.16 shows one such chunk, describing the racers and league standings.</p>
<div class="highlight"><pre><span></span>During the 2023 Pod Racing season, the following racers competed:
* Senn Kava piloting The Thunderbolt.
* Vix Tor piloting The Shadow Racer.
* Remy Thal piloting The Crimson Fang.
* Tira Suro piloting The Lightning Bolt.
* Kael Voss piloting The Razor Blade.

Anakin Skywalker was mysteriously absent from this season.

The final standings:
* Senn Kava won the cup.
* Vix Tor came in second.
* Remy Thal was third.
* Tira Suro took forth place.
* Kael Voss was last.

The racing season consisted of 5 races:
* The Tatooine Grand Prix (race 1)
* The Coruscant Circuit (race 2)
* The Naboo Invitational (race 3)
* The Genosis Challenge (race 4)
* The Bespin Cup (race 5)
</pre></div>

<p><em>Listing 4.16: Pod Racing league facts from which we generate training data.</em></p>

<p>The Pod Racing league fact files are available in the GitHub repo under the
<code>/code/racing/</code> folder. The folder contains 6 files, <code>league.txt</code>, <code>race1.txt</code>,
<code>race2.txt</code>, <code>race3.txt</code>, <code>race4.txt</code>, and <code>race5.txt</code>. <code>league.txt</code> is shown in
listing 4.16. The remaining files contain facts about each race. Listing 4.17
shows the content of <code>race1.txt</code>, so you get an idea of how the dataset looks
like.</p>
<div class="highlight"><pre><span></span>During the Tatooine Grand Prix Pod Racing race, there were several thrilling and
unexpected moments that led to the final standings:

Thunderous Takeoff: At the starting line, Senn Kava&#39;s Thunderbolt pod launched
with an extraordinary burst of speed, leaving the other racers in awe. The
expertly tuned engine of the Thunderbolt provided an unprecedented acceleration
advantage, allowing Kava to establish a commanding lead right from the
beginning.

Shadow Racer&#39;s Slipstream: As the race progressed, Vix Tor&#39;s Shadow Racer
skillfully utilized the aerodynamic properties of the pod to ride the slipstream
created by the Thunderbolt. By closely tailing behind Kava, Tor minimized air
resistance and gained a significant speed boost, catapulting the Shadow Racer
into the top three positions.

Thrilling Aerial Maneuvers: Remy Thal, piloting the Crimson Fang, exhibited
exceptional piloting skills during a treacherous section of the racecourse. Thal
expertly navigated through a series of tight turns and dangerous obstacles,
executing daring mid-air flips and barrel rolls. The crowd held their breath as
the Crimson Fang narrowly avoided collisions, ultimately propelling Thal into
second place.

Lightning Bolt&#39;s Electrifying Boost: Tira Suro&#39;s Lightning Bolt had a unique
energy-harvesting system integrated into its design. During the race, Suro
strategically positioned the pod near an electrical storm that unexpectedly
formed on the course. The Lightning Bolt harnessed the electrical energy,
momentarily activating a boost of unparalleled speed. However, the power surge
was short-lived, and Suro slipped to fourth place after the energy dissipated.

Razor Blade&#39;s Perilous Gamble: Kael Voss, piloting the Razor Blade, took a
daring risk to gain an advantage during the race. In a section with a hazardous
sandstorm, Voss decided to take an alternate route through a rocky canyon. While
this path appeared shorter, it was fraught with perilous terrain and
unpredictable gusts of wind. Despite the danger, Voss showcased remarkable
precision and nerve, allowing the Razor Blade to secure the fifth-place
position.

These exhilarating moments and unexpected tactics turned the Tatooine Grand Prix
Pod Racing race into a thrilling spectacle for spectators, leaving them on the
edge of their seats until the checkered flag was waved.
</pre></div>

<p><em>Listing 4.17: Pod Racing league facts from which we generate training data.</em></p>

<p>Note each paragraph talks about a pilot and how they performed during the race.
The remaining files follow the same format.</p>

<p>The key takeaway is we can take an arbitrarily large dataset, chunk it, generate
prompts and completions from it, then use them to fine-tune a model so it can
answer domain-specific questions.</p>

<p>Listing 4.18 shows how we can get JSONL using <code>gpt-3.5-turbo</code>.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llm_utils</span> <span class="kn">import</span> <span class="n">ChatTemplate</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">template</span> <span class="o">=</span> <span class="n">ChatTemplate</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s1">&#39;jsonl.json&#39;</span><span class="p">)</span>
<span class="n">jsonl</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;../racing&#39;</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;../racing&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">completion</span><span class="p">({</span><span class="s1">&#39;info&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span> <span class="s1">&#39;20&#39;</span><span class="p">})</span>
    <span class="n">jsonl</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;racing.jsonl&#39;</span><span class="p">,</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">jsonl</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.18: Generating prompts and completions from some facts.</em></p>

<p>We list all files in the <code>/racing</code> subfolder, then user our <code>jsonl.json</code> prompt
to get <code>gpt-3.5-turbo</code> to produce JSONL prompt and completion pairs. We collect
all of these in a <code>jsonl</code> string which we finally write to a file,
<code>racing.jsonl</code>.</p>

<p>Running this should return a list of prompts and completions formatted as JSONL,
based on the facts we passed in. Listing 4.19 shows the first few items:</p>
<div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What maneuver did Remy Thal execute during the Bespin Cup Pod Racing race?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;A gravity-defying maneuver utilizing the pod&#39;s advanced repulsorlift technology.&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Who piloted the Crimson Fang during the Bespin Cup Pod Racing race?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Remy Thal.&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What strategy did Vix Tor employ during the Bespin Cup Pod Racing race?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;completion&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;An elusive approach that relied on the pod&#39;s exceptional agility and maneuverability.&quot;</span><span class="p">}</span>
</pre></div>

<p><em>Listing 4.19: Part of the response we get when running listing 4.17.</em></p>

<p>The full output is available on GitHub as <code>racing.jsonl</code>.</p>

<p>Figure 4.3 shows the steps we just went through.</p>

<p><img src="images/04/fig3.png" alt="Figure 4.3"></p>

<p><em>Figure 4.3: Preparing the data for fine-tuning.</em></p>

<p>We take as many chunks of data as we have (in our case the various text files),
we inject them in the JSONL prompt, and have <code>gpt-3.5-turbo</code> produce
prompt/completion JSONL. We concatenate the outputs into our dataset.</p>

<p>We will use the OpenAI command line tools for fine-tuning. These should‚Äôve been
installed when we ran <code>pip install openai</code> all the way back in chapter 1. Before
fine-tuning, we‚Äôll use the command line to double-check our JSONL data. Listing
4.20 shows the command.</p>
<div class="highlight"><pre><span></span>openai<span class="w"> </span>tools<span class="w"> </span>fine_tunes.prepare_data<span class="w"> </span>-f<span class="w"> </span>racing.jsonl
</pre></div>

<p><em>Listing 4.20: Using OpenAI command line to prepare the JSONL data for
fine-tuning.</em></p>

<p>This should output an analysis of the dataset and suggestions. For this
particular dataset, the suggestion would be that completions should start with a
whitespace character (they don‚Äôt in our case). The tool offers to automatically
fix this, and outputs <code>racing_prepared.jsonl</code>. Remember to always run this
command before fine-tuning to make sure the data is valid.</p>

<p>We now have our fine-tuning data ready. The next step is to use it on a based
model and get our 2023 Pod Racing Q&amp;A model.</p>

<h3>Fine-tuning the model</h3>

<p>We‚Äôll be using the command line tools for fine-tuning. Listing 4.21 shows the
command to start the fine-tuning process.</p>
<div class="highlight"><pre><span></span>openai<span class="w"> </span>api<span class="w"> </span>fine_tunes.create<span class="w"> </span>-t<span class="w"> </span>racing_prepared.jsonl<span class="w"> </span>-m<span class="w"> </span>davinci
</pre></div>

<p><em>Listing 4.21: Using OpenAI command line to fine-tune a model.</em></p>

<p>This will upload the JSNOL training data and start a fine-tuning job. The output
should contain a unique identifier for the job, something like
<code>ft-FYTbxinEtM7UeJb0Uz4P6kS3</code>. The <code>ft-</code> part stands for fine-tune, the rest is
a unique identifier.</p>

<p>The job will be queued by OpenAI and you should see console updates with the
place in the queue etc. If for whatever reason the connection is lost, you can
re-connect using the command in listing 4.22.</p>
<div class="highlight"><pre><span></span>openai<span class="w"> </span>api<span class="w"> </span>fine_tunes.follow<span class="w"> </span>-i<span class="w"> </span>ft-FYTbxinEtM7UeJb0Uz4P6kS3
</pre></div>

<p><em>Listing 4.22: Using OpenAI command line to follow a fine-tuning job.</em></p>

<p>Update the unique identifier with the one you got after running the code in
listing 4.21. The fine-tuning job will eventually execute and will result in a
fine-tuned model you can subsequently use in your application. Listing 4.23
shows an example of the last lines of output from the fine-tuning job.</p>
<div class="highlight"><pre><span></span>Job complete! Status: succeeded üéâ
Try out your fine-tuned model:
openai api completions.create -m davinci:ft-personal-2023-06-19-17-42-56 -p &lt;YOUR_PROMPT&gt;
</pre></div>

<p><em>Listing 4.23: Output once model fine-tuning job is complete.</em></p>

<p>In this example, the model name is <code>davinci:ft-personal-2023-06-19-17-42-56</code>.
The exact model name will be different for you when you run the job, but it
follows the same pattern: the base model (in our case <code>davinci</code>), followed by
<code>ft</code> (fine-tune), <code>personal</code> stands for using a personal account with OpenAI,
and a timestamp representing the date and time when this model was created.</p>

<p>We took multiple datasets, combined them into our JSONL data, ran a fine-tuning
job for a base model, and we ended up with a fine-tuned model we can now prompt
and get responses from (see figure 4.4).</p>

<p><img src="images/04/fig4.png" alt="Figure 4.4"></p>

<p><em>Figure 4.4: Fine-tuning a model with a prompt/completion dataset.</em></p>

<p>Note this fine-tuned model is tied to your account ‚Äì it is not publicly
available. You can access it with your account but a different person with a
separate account will not have access to it.</p>

<p>Now let‚Äôs see how we can use this model.</p>

<h3>Using the model</h3>

<p>We‚Äôll implement another chat ‚Äì an interactive Q&amp;A with the fine-tuned model.
Listing 4.24 shows the code for this.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span>

<span class="n">template</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;You are a Q&amp;A bot. You provide short answers to questions.</span>
<span class="s1">For example:</span>
<span class="s1">Question: Who was missing from this season? Anakin Skywalker.</span>
<span class="s1">Provide the answer to the following question:</span>
<span class="s1">Question: &#39;&#39;&#39;</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s1">&#39;user: &#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="o">==</span> <span class="s1">&#39;exit&#39;</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s1">&#39;davinci:ft-personal-2023-06-19-17-42-56&#39;</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">],</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">template</span> <span class="o">+</span> <span class="n">prompt</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>

<p><em>Listing 4.24: Q&amp;A using the fine-tuned model.</em></p>

<p>We‚Äôre not using <code>llm_utils</code> in this case since our <code>Template</code> class uses a
hardcoded model ‚Äì for good reason, since this is the only code sample where
we‚Äôre using a different model.</p>

<p>We have a template providing an example question and answer (one-shot learning)
for formatting. Note that we are using the fine-tuned model
<code>davinci:ft-personal-2023-06-19-17-42-56</code>. We‚Äôre also providing a <code>stop</code>
argument, to stop after a newline. I observed the model tends to continue
outputting text even after it answers the question. This <code>stop</code> argument
combined with the one-shot example in the prompt helps getting better output.
Listing 4.25 shows an example Q&amp;A with this model.</p>
<div class="highlight"><pre><span></span>Crimson Fang.
user: Who pilots The Razor Blade?
 Kael Voss.
user: What happened to Senn Kava during the Genosis Challange?
 An unexpected power surge caused the pod&#39;s stabilization system to fail.
</pre></div>

<p><em>Listing 4.25: Example Q&amp;A interaction.</em></p>

<p>As we can see, the fine-tuned model remembered the facts we fed it and answers
the questions correctly.</p>

<p>We just saw how we can fine-tune a base model with a custom dataset and how to
integrate that model in an application ‚Äì Q&amp;A in our case.</p>

<h3>Additional considerations</h3>

<p>Fine-tuning performs better the larger the training dataset (the
prompt/completion JSONL), at least a few hundred examples. In our case, since
we‚Äôre dealing with a toy example, the dataset is rather small, so model
performance won‚Äôt be great. In practice, you would want to use larger datasets ‚Äì
the larger the better.</p>

<p>Another best practice, which we didn‚Äôt do to keep things concise, is to train
the model to say it doesn‚Äôt know when the answer is not available in the
dataset. These are called <em>adversarial examples</em>. Ideally our JSONL should
contain prompts for facts not captured in the data we have, and completions
saying something like ‚ÄúI am sorry, I do not have this data‚Äù or ‚ÄúI don‚Äôt have the
answer for this‚Äù or something among those lines. If we don‚Äôt provide such
examples, the model will make up answers, even if it doesn‚Äôt have the facts,
something we would like to avoid.</p>

<p>Fine-tuning a model doesn‚Äôt have to be a one-time thing. We can take a
fine-tuned model and fine-tune it further with additional data. Figure 4.5 shows
how this works.</p>

<p><img src="images/04/fig5.png" alt="Figure 4.5"></p>

<p><em>Figure 4.5: Fine-tuning a fine-tuned model.</em></p>

<p>Refer to the OpenAI documentation for more details on fine-tuning:
<a href="https://platform.openai.com/docs/guides/fine-tuning">https://platform.openai.com/docs/guides/fine-tuning</a>.</p>

<p>Let‚Äôs also talk about some of the drawbacks of fine-tuning, and why we might
want to consider alternative approaches.</p>

<p>First of all, fine-tuning means taking a base model and adding parameters to it
based on the dataset we‚Äôre using for the training job. This model will be
different from the model we started with. If you are working on a large system
that relies on large language models in multiple places, each using a
differently fine-tuned model, the models will diverge in the responses they
provide as they no longer have the same parameters. This might or might not be
an issue depending on your specific use-case, but something to keep in mind.</p>

<p>More importantly, at the time of writing, OpenAI just added support for
fine-tuning GPT-3.5 Turbo models. Previously, only GPT-3 models were
fine-tunable, even though there was a new generation of models available. Due to
popular demand, <code>gpt-3.5-turbo-instruct</code> is a model that supports fine-tuning.
That said, the best performing model now is GPT-4. In general, we‚Äôll be getting
worse responses from older models than we would from newer models.
Unfortunately, the newest models do not support fine-tuning.</p>

<p>This seems like a pretty bad tradeoff. We‚Äôd like to use a large language model
with better performance. But remember, we went through the trouble of
fine-tuning a model because we had a dataset too large to fit into a prompt.
Turns out there is an alternative: what if we had a way to select only the
relevant information from a large dataset and inject it in the prompt, sticking
to the token limits? The next chapter covers exactly this scenario: using memory
instead of fine-tuning.</p>

<h2 id="summary">Summary</h2>

<ul>
<li>Large language models can perform, when prompted, tasks for which they weren‚Äôt
specifically trained ‚Äì this is called <em>zero-shot learning</em>.</li>
<li>Often, a single example is enough to get desired behavior, this is <em>one-shot
learning</em>.</li>
<li>Use one-shot learning when it‚Äôs easier to show than to tell how a response
should look like.</li>
<li>In some cases, a single example is not enough to elicit the desired behavior,
we need to provide multiple examples ‚Äì this is called <em>few-shot learning</em>.</li>
<li>If we have a dataset too large to fit in a prompt, we can <em>fine-tune</em> a model
on the dataset.</li>
<li>Fine-tuning takes a base model and trains it (adding additional parameters) on
the dataset provided.</li>
<li>We can take a fine-tuned model and fine-tune it further with additional data.</li>
<li>The larger the dataset, the better responses we will get.</li>
<li>Provide adversarial examples, otherwise the models will hallucinate responses
even when lacking data.</li>
</ul>

</article>
<nav>
<p>

<span>Previous: <a href="prompt-engineering.html">Prompt Engineering</a>. 


<span>Next: <a href="memory-and-embeddings.html">Memory and Embeddings</a>.

</p>
</nav>
<footer><span>By <a href="https://vladris.com/">Vlad Ri»ôcu»õia</a>&nbsp;|&nbsp;<a href="https://github.com/vladris/llm-book/issues/new">üì£ Feedback</a>&nbsp;|&nbsp;<a href="https://tinyletter.com/vladris">‚úâÔ∏è Subscribe</a></span></footer>
</body>
</html>